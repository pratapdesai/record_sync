Record Sync Service

## 1. Overview
The Record Sync Service is a scalable, event-driven, decoupled microservice built to synchronize CRUD (Create, Read, Update, Delete) record operations between:

System A (internal system, full control)
System B (external CRMs like Salesforce, Outreach, HubSpot)

It provides:

* Reliable near-real-time synchronization
* Extensible support for multiple CRMs
* Robust error handling
* Dynamic rules-based decision-making
* Pluggable data transformations
* Rate-limit awareness
* Queue/buffer mechanisms to throttle external calls
* Observability and retry/fallback capabilities

The service is designed for high scale (300 million records/day) with 99.9% availability targets.

### Architecture

System A  <-----(`API Key`)---->  Record Sync Service  <---(`JWT`)--> System B (CRM)


## 2. Key components:

- FastAPI API layer with strong data validation (Pydantic)

- Queue manager (Redis, or mocked Redis for local dev)

- Rules engine to evaluate CRUD triggers

- Transform layer to map internal to external schemas

- CRM plugins using Strategy Pattern

- Status tracker to manage record states and retries

- Config manager with dynamic override support

- Retry manager with circuit breaker style

- Rate limiter to handle external API limits

- JSON logs for observability

- Extensible for Outreach-style sync patterns

- Dynamic rules update through an API

- No data loss guarantee through retries and idempotency

## 3. Technology Choices

‚úÖ Python 3.10

‚úÖ FastAPI

- async-native, high concurrency

- Pydantic schemas

- easy testability

- production performance close to Node/Go

- simpler than Django

‚úÖ Redis (Elasticache)

- fast in-memory buffer

- handles massive scale

- supports rate throttling with sliding window

‚úÖ Postgres (optional future)

- for persistent idempotency store

- status tracking

‚úÖ Loguru

- JSON structured logs

- easy ingestion to CloudWatch or ELK

‚úÖ Tenacity

- advanced retry/backoff

## 4. Patterns Used
| Pattern |  Usage |
| ---------------- | --------------|
| Strategy Pattern |  each CRM plugin (SalesforceCRM, OutreachCRM) implements its own push() and transform() logic |
| Singleton | ConfigManager uses a singleton to manage dynamic configuration |
|Command pattern | each record sync is a command event
| Rules Engine | applies business rules to determine whether to sync
| Rate limiting | a sliding window algorithm with queue throttling
|Circuit breaker| uses retry libraries with capped attempts
|Queue buffer | Redis (or mocked in-memory) to handle burst loads
|Idempotency| record IDs are used as unique keys
|Logs |structured JSON logs (Loguru) for CloudWatch or ELK parsing

## 5. High-Level Flow

- System A sends a sync request to POST `/v1/sync/`

- The RulesEngine evaluates whether to proceed based on preconfigured rules (required fields, disallowed fields, etc.)
  - If permitted, the record is queued

- Queue Manager periodically flushes batches

- Rate limiter ensures CRM API limits are respected and controls how many records per minute flush

- CRM plugin (transform() + push()) pushes data to external CRM 
- CRM plugin uses JWT to push to System B 
- Transform maps fields from System A to CRM 
- Status is tracked and updated (queued, synced, failed, etc.)
- Retries are scheduled if a transient failure happens. Retry manager handles retries with backoff 
- Manual retries can be triggered 
- All events logged for observability 
- All logs are JSON structured 
- Rules can be edited dynamically via the `/v1/sync/rules` endpoint 
- Configs can be overridden dynamically via `/v1/sync/config-override`

## 6. API Design
| Method | Endpoint | Purpose |
| --------------- | --------------| -------------|
|POST	|/v1/sync/	|Submit a sync event (CRUD driven by operation)
|POST	|/v1/sync/retry/{record_id}	|Manually retry a failed sync
|POST	|/v1/sync/config-override	|Dynamically override CRM configurations
|GET	|/v1/sync/status/{record_id}|	Get current sync status
|POST	|/v1/sync/rules|	Dynamically update sync rules (without S3)

## 7. Rules Engine

* evaluated on each sync request
* rules defined in rules.json
* supports required fields and disallowed conditions
* Supports dynamic rule updates via the API (`/v1/sync/rules`)
* Future
  * Rules.json can be pulled from S3 and rules_engine_s3.py provides interfaces to pull rules from S3
  * Can easily be moved to Postgres for durability later


## 8. Transform Layer

- Each CRM plugin provides a transform() method
- Maps internal schema fields to the CRM‚Äôs expected fields
- Example: first_name ‚Üí FirstName for Salesforce 
- This is flexible and future-proof, supporting:
  - new fields 
  - versioned CRM APIs 
  - schema migrations

## 9. Rate Limiting

- Sliding window algorithm per CRM 
- Flushes queued records in controlled batches 
- Each CRM has :
  - configurable `batch_size`
  - configurable `flush_interval`
  - configurable `rate_limit_per_minute`

- Dynamic override API supports changing the CRM rate limiting configs at runtime


## 10. Retry and Circuit Breaker
- transient failures retried 3 times 
- exponential backoff with Tenacity 
- circuit breaker: prevents flooding CRM on repeat failures 
- manual retry endpoint for user-triggered reattempt for on demand retries.
- permanent validation errors (missing required field, etc.) are not retried

## 11. Status Tracking
 - StatusManager tracks status of each record in memory

 - Can easily migrate to Postgres later

 - Provides a GET `/v1/sync/status/{record_id}` to inspect 
   - queued 
   - synced 
   - failed 
   - skipped_by_rule

## 12. Observability

- Structured JSON logs via Loguru

- Logs written to file and console

- Easily ingestible by CloudWatch, ELK, or Splunk

- Stats can be exported to metrics if needed

- Hooks for OpenTelemetry tracing ready to add

## 13. Dynamic Config
 - ConfigManager uses config.ini to load defaults

- `/v1/sync/config-override` API lets you change:
  - batch_size 
  - flush_interval 
  - rate limits 
  - API hosts

- New configs are stored in memory

- A new batch will pick them up

## 14. Scaling Considerations
 - 300 million records/day target
- horizontally scalable FastAPI containers on ECS Fargate
- Redis cluster Can be integrated to handle massive throughput
- RDS Postgres can be considered for durable metadata 
- CloudWatch for metrics/alarms 
- SecretManager  Store for secure secrets 
- LB in front of FastAPI 
- SQS could also be plugged for decoupling 
- Kinesis/Kafka if event fan-out is needed

## 15. Security

- Internal System A authenticates using API keys

- record_sync_service authenticates to CRM via JWT (or OAuth)

- Rules update API should be protected with a strong admin API key or OAuth (planned)
- Config override also to be protected
- No sensitive secrets in code

## 16. Extensibility

Adding a new CRM:
- create a new MyCRM subclass of BaseCRM

- define:

  - identify()
  - transform()
  - push()

- register in SyncManager

- done!

## 17. Trade-offs

‚úÖ Chose single sync endpoint:

- Why you don‚Äôt see explicit CRUD?
  - explicit CRUD routes would be more RESTful but harder to batch
  - In the code so far, the operation field on the sync requests carries the CRUD intention (create, read, update, delete) ‚Äî but did not explicitly break them into separate routes, because ‚Äúsingle sync endpoint‚Äù pattern is being followed that takes the operation type in the payload and then applies a transformation.
  
- easier batching

- simpler idempotency

- fewer routing changes if new operations arise

‚úÖ Separate CRUD routes would add discoverability, but reduce batching benefits
‚Üí event-driven is better here

‚úÖ Used local rules.json:

easier to reason about

easier to test

no cross-region S3 worries

‚úÖ Redis for buffer:

in-memory, super fast

but you need persistence if you truly cannot lose records (append to disk or use Redis Streams in prod)

## 18. Testing & Quality

- `pytest` test suite included

- black + isort for consistent formatting

- strong Pydantic typing and type-annotated everywhere
- environment driven for secrets 
- logs show clear queue/batch/flush actions
- rate limiter tested

  ### Dockerized
  - Entire code is properly dockerized to ease the execution.
  - clear, well-separated module structure


## 19. Commands

  ### How to Run
  ```
  make run
  ```
  ### How to Test

  ```
  make test
  ```
  ### How to Build Docker
  ```
  docker build -t record-sync .
  docker run -p 8000:8000 record-sync
  ```

## 20. Future Improvements

‚úÖ persist rules to Postgres instead of a local file \
‚úÖ add a feature flag system (e.g. LaunchDarkly) \
‚úÖ better observability with Prometheus + Grafana \
‚úÖ move to Redis Streams for ordered queues \
‚úÖ allow multi-tenant CRM logic \
‚úÖ schema-driven transforms via JSON schemas instead of code \
‚úÖ integrate KMS for secret encryption 

## 21. üöÄ Conclusion

This design is production-grade, highly extensible, observable, and easy to scale. It fits a Staff-level engineering exercise with a modern, microservice, event-driven architecture, applying: 
 - best practices (patterns, error handling, rate limiting)
 - clean, type-safe API contracts 
 - clear decoupling 
 - clean paths for future enhancements

